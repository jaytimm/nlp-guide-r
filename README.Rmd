---
output:
  md_document:
    variant: markdown_github
---

# NLP with R: some notes



A summary of some R-based, NLP workflows.  I principally use the `udpipe` (!) package for working with text data.  It is a beast, and it keeps things simple from a data class perspective (ie, data frames only).  For tokenization tasks and quick text search, I use the `corpus` package.  `text2vec` is a lovely package as well, and is super useful for building some common NLP data structures (eg, TCMs & DTMs) quickly.  I have tried them all, and these are the guys I have landed on. I am a picky linguist, and they are flexible & lightweight.  Mostly a resource for self.  




---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("/home/jtimm/pCloudDrive/GitHub/git-projects/render_toc.R")
```



```{r echo=FALSE}
render_toc("/home/jtimm/pCloudDrive/GitHub/nlp-cheat-r/README.Rmd")
```


## Quick live text

### Online news articles

```{r message=FALSE, warning=FALSE}
library(tidyverse)
meta <- quicknews::qnews_get_newsmeta()
news <- quicknews::qnews_extract_article(url = meta$link,
                                         cores = 7)
```



### PubMed abstracts

```{r eval=FALSE}
s0 <- PubmedMTK::pmtk_search_pubmed(search_term = 'medical marijuana', 
                                    fields = c('TIAB','MH'))

s1 <- PubmedMTK::pmtk_get_records2(pmids = s0$pmid, 
                                   cores = 3 #, 
                                   #ncbi_key = key
                                   )
```


### Tweets

```{r}

```




## Processing

### Tokenization

```{r}
a1 <- corpus::text_tokens(news$text,
                          
  filter = corpus::text_filter(
    map_case = TRUE, 
    map_quote = TRUE,
    remove_ignorable = TRUE,
    combine = c(corpus::abbreviations_en),
    stemmer = NULL,
    stem_dropped = FALSE,
    stem_except = NULL,
    drop_letter = FALSE,
    drop_number = FALSE,
    drop_punct = FALSE,
    drop_symbol = FALSE,
    drop = NULL,
    drop_except = NULL,
    connector = '_',
    sent_crlf = FALSE)
  )

names(a1) <- 1:nrow(news)
```


### Sentence tokenization


### Annotation

```{r include=FALSE}
udmodel_dir <- '/home/jtimm/pCloudDrive/GitHub/packages/biberizer/'
```


```{r}
setwd(paste0(udmodel_dir, 'model'))
udmodel <- udpipe::udpipe_load_model('english-ewt-ud-2.3-181115.udpipe')

x0 <- udpipe::udpipe(object = udmodel,
                     x = a1[1:10],
                     tagger = 'default', #'none'
                     parser = 'none')
```



## Multi-word expressions

### Collocations

```{r}
collocations <- udpipe::collocation(x = x0,
                                    term = 'token',
                                    group = c('doc_id'),
                                    ngram_max = 5,
                                    sep = ' ')

collocations0 <- subset(collocations, freq > 5 & pmi > 8 &
                          !grepl('[[:punct:]]', keyword))

collocations0 %>% sample_n(6) %>%
  mutate(pmi = round(pmi, 3)) %>%
  select(keyword, freq, pmi) %>%
  knitr::kable()
```




### Noun phrases

```{r message=FALSE, warning=FALSE}
x0$phrase_tag <- udpipe::as_phrasemachine(x0$xpos, 
                                          type = "penn-treebank")

splits <- split(x0, f = x0$doc_id)

nps <- lapply(1:length(splits), function(x) {
  udpipe::keywords_phrases(x = splits[[x]]$phrase_tag,
                           term = splits[[x]]$token,
                           pattern = "(A|N)+N(P+D*(A|N)*N)*",
                           is_regex = TRUE,
                           ngram_max = 5,
                           detailed = TRUE,
                           sep = '_') })

names(nps) <- names(splits)
nps1 <- data.table::rbindlist(nps, idcol = 'doc_id')

nps1 %>%
  count(keyword, pattern, ngram) %>%
  sample_n(5) %>%
  knitr::kable()
```


### Tokenizing multi-word expressions

```{r}
# lex$ngram <- stringr::str_count(lex$TermName,stringr::fixed('_')) + 1
# data.table::setDT(lex)
# ms <- subset(lex, lex$ngram > 1)

x0$newness <- udpipe::txt_recode_ngram(tolower(x0$token),
                                       compound = c(nps1$keyword),
                                       ngram = c(nps1$ngram),
                                       sep = '_')

x0 %>%
  select(doc_id, token:xpos, newness) %>%
  head() %>%
  knitr::kable()
```


### Dictionary-based entity recognition 

### Rebuilding text



## doc2vec

## Search

### Search in context

### Highlight 

### More complex patterns

## Odds

### Visualizing dependencies



