---
output:
  md_document:
    variant: markdown_github
---

# nlp with r: some notes

A summary of some (more upstream) NLP workflows -- mostly using the [udpipe](https://github.com/bnosac/udpipe) and [corpus](https://github.com/patperry/r-corpus) packages.  Mostly notes to self.

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("/home/jtimm/pCloudDrive/GitHub/git-projects/render_toc.R")
```



```{r echo=FALSE}
render_toc("/home/jtimm/pCloudDrive/GitHub/git-projects/nlp-cheat-r/README.Rmd")
```


## Quick live text

### Online news articles

```{r message=FALSE, warning=FALSE}
library(tidyverse)
rss1 <- quicknews::qnews_build_rss(x = 'political ideology')
meta <- quicknews::qnews_strip_rss(rss1) 
news <- quicknews::qnews_extract_article(url = meta$link[1:10], cores = 7)

full <- news %>% left_join(meta)

strwrap(full$text[1], width = 60)[1:5]
```



### PubMed abstracts

```{r}
pmids <- PubmedMTK::pmtk_search_pubmed(search_term = 'political ideology', 
                                       fields = c('TIAB','MH'))

abstracts <- PubmedMTK::pmtk_get_records2(pmids = pmids$pmid[1:10], 
                                          cores = 3 #, 
                                          #ncbi_key = key
                                          )

strwrap(abstracts[[1]]$abstract, width = 60)[1:10]
```


### Tweets

```{r}
tweets <-  rtweet::search_tweets(q = 'political ideology',
                                 n = 100,
                                 type = "recent",
                                 include_rts = FALSE,
                                 geocode = NULL,
                                 max_id = NULL,
                                 parse = TRUE,
                                 token = NULL)

strwrap(tweets$text[1], width = 60)
```




## Processing

### Sentence splitting

```{r}
abbrevs <- c(corpus::abbreviations_en, 'Gov.', 'Sen.')
abbrevs
```


```{r}
c0 <- full$text
names(c0) <- full$doc_id

sentences <- corpus::text_split(c0, 
                                filter = corpus::text_filter(
                                  sent_suppress = abbrevs))

sentences$text <- as.character(sentences$text)
sentences$uid <- paste0(sentences$parent, '.', sentences$index)
colnames(sentences)[1:2] <- c('doc_id', 'sentence_id')

sentences %>% select(sentence_id, text) %>% head() %>% knitr::kable()
```




### Tokenization

> The `text_tokens` function from the `corpus` package provides a host of options for text tokenization.  

```{r}
tokens <- corpus::text_tokens(sentences$text,
                          
  filter = corpus::text_filter(
    map_case = FALSE, 
    map_quote = TRUE,
    remove_ignorable = TRUE,
    combine = abbrevs,
    stemmer = NULL,
    stem_dropped = FALSE,
    stem_except = NULL,
    drop_letter = FALSE,
    drop_number = FALSE,
    drop_punct = FALSE,
    drop_symbol = FALSE,
    drop = NULL,
    drop_except = NULL,
    connector = '_',
    sent_crlf = FALSE)
  )

names(tokens) <-sentences$uid
tokens[[1]]
```



### Tokens to data frame

> A simple approach to reshaping token objects.  Via the `textshape` package.

```{r}
df <- textshape::tidy_list(tokens, 
                           id.name = 'doc_id', 
                           content.name = 'token')
  
####
df[, sentence_id := gsub('^.*\\.', '', doc_id)]
df[, doc_id := gsub('\\..*$', '', doc_id)]
df[, token_id := data.table::rowid(doc_id)]

df %>%  slice(1:10)
```



### Vocabulary

```{r}
vocab <- df[, list(text_freq = .N, 
                          doc_freq = length(unique(doc_id))), 
              by = list(token)]

head(vocab)
```



## Annotation

```{r include=FALSE}
udmodel_dir <- '/home/jtimm/pCloudDrive/GitHub/packages/biberizer/'
```


```{r message=FALSE, warning=FALSE}
setwd(paste0(udmodel_dir, 'model'))
udmodel <- udpipe::udpipe_load_model('english-ewt-ud-2.3-181115.udpipe')
```


> The `udpipe` package can be used to annotate simple text or token objects.  One benefit of annotating a token object versus simple text is that the user can define token and sentence constitution.

```{r}
tokens1 <- lapply(tokens, c, '\n')
names(tokens1) <- gsub('\\..*$', '', names(tokens1))

tokens2 <- sapply(unique(names(tokens1)), 
       function(z) unname(unlist(tokens1[names(tokens1) == z])), 
       simplify=FALSE)
```



```{r message=FALSE, warning=FALSE}
annotation <- udpipe::udpipe(object = udmodel,
                             x = tokens2,
                             tagger = 'default', 
                             parser = 'default')

colnames(annotation)
```



```{r}
annotation %>%
  select(doc_id, sentence_id, token_id:xpos) %>%
  head() %>%
  knitr::kable()
```




## Multiword expressions

### Collocations

```{r}
collocations <- udpipe::collocation(x = annotation,
                                    term = 'token',
                                    group = c('doc_id'),
                                    ngram_max = 5,
                                    sep = ' ')

collocations0 <- subset(collocations, freq > 1 & pmi > 5 &
                          !grepl('[[:punct:]]', keyword))

collocations0 %>% 
  sample_n(6) %>%
  mutate(pmi = round(pmi, 3)) %>%
  select(keyword, freq, pmi) %>%
  knitr::kable()
```




### Noun phrases

```{r message=FALSE, warning=FALSE}
annotation$phrase_tag <- udpipe::as_phrasemachine(annotation$xpos, 
                                                  type = "penn-treebank")

splits <- split(annotation, f = annotation$doc_id)

## lapply to preserve doc_id info
nps <- lapply(1:length(splits), function(x) {
  udpipe::keywords_phrases(x = splits[[x]]$phrase_tag,
                           term = splits[[x]]$token,
                           pattern = "(A|N)+N(P+D*(A|N)*N)*",
                           is_regex = TRUE,
                           ngram_max = 5,
                           detailed = TRUE,
                           sep = '_') })

names(nps) <- names(splits)
nps1 <- data.table::rbindlist(nps, idcol = 'doc_id')

nps1 %>%
  count(keyword, pattern, ngram) %>%
  sample_n(5) %>%
  knitr::kable()
```




### Tokenizing multiword expressions

> Recode noun phrases identified above as a single token in annotation data frame.

```{r}
# lex$ngram <- stringr::str_count(lex$TermName,stringr::fixed('_')) + 1
# data.table::setDT(lex)
# ms <- subset(lex, lex$ngram > 1)

annotation$newness <- udpipe::txt_recode_ngram(tolower(annotation$token),
                                               compound = c(nps1$keyword),
                                               ngram = c(nps1$ngram),
                                               sep = '_')
```



## Annotation to DTM

> Per the annotation structure above, we can (1) cast into a document-term matrix and (2) normalize vocabulary to the lemma in one fell swoop.

```{r}
annotation0 <- annotation %>%
  filter(!is.na(newness)) %>%
  mutate(newness = ifelse(grepl('_', newness), newness, lemma),
         uid = paste0(doc_id, '.', sentence_id)) 

dtm <- annotation0 %>% 
  count(doc_id, newness) %>%
  tidytext::cast_sparse(row = doc_id,
                        column = newness,
                        value = n)
str(dtm)
```



## doc2vec

```{r}
new_text <- data.table::setDT(annotation0)[, list(text = paste(newness, collapse = " ")), 
                                  by = doc_id]

strwrap(new_text$text[1], width = 60)[1:5]
```


```{r}
new_text$nwords <- tokenizers::count_words(new_text$text)
new_text0 <- subset(new_text, nwords < 1000 & nchar(text) > 0)

set.seed(9)
model.d2v <- doc2vec::paragraph2vec(x = new_text0, 
                                    type = "PV-DM", 
                                    dim = 100, 
                                    iter = 20,
                                    min_count = 2, 
                                    lr = 0.05, 
                                    threads = 1)

embedding.words <- as.matrix(model.d2v, which = "words")
embedding.docs <- as.matrix(model.d2v,   which = "docs")

both <- do.call(rbind, list(embedding.docs, embedding.words))
```


> doc2vec is a powerful NLP tool because it projects documents and terms in the same embedding space.  

```{r}
predict(model.d2v, 'find', 
        type = "nearest",
        which = "word2word")[[1]]
```



## Text summary via Pagerank

> Here, we use the annotated version of our corpus, and filter PoS to nouns and adjectives; however, non-annotated version of corpus could be used. Via the `textrank` package.

```{r}
sent1 <- sentences[, c('sentence_id', 
                       'text', 
                       'doc_id', 
                       'uid')]

df1 <- subset(annotation0[, c('sentence_id', 
                              'lemma',
                              'upos',
                              'doc_id', 
                              'uid')],
              upos %in% c('PROPN', 'NOUN', 'ADJ'))

tr <- lapply(unique(df1$doc_id), function(x){
  
  ss <- subset(sent1, doc_id == x)
  tm <- subset(df1, doc_id == x)
  if(nrow(tm) < 4 | nrow(ss) < 4) {NA} else{
    textrank::textrank_sentences(data = ss[, c(1:2)], 
                                 terminology = tm[, c(1:2)]) }   })

s <- lapply(tr, 
            summary, 
            n = 3, 
            keep.sentence.order = TRUE)

names(s) <- full$title
s[1:3]
```



## Search

### Search in context

```{r}
ctrialsgov::ctgov_kwic(term = 'political ideology|party affiliation', 
                       text = abstracts[[1]]$abstract, 
                       names = abstracts[[1]]$pmid, 
                       width = 35,
                       #use_color = T,
                       output = 'cat') #'data.frame'
```



### Sentences containing X

```{r}
eg_sentences <- df[, if(any(token %in% c('QAnon'))) .SD, 
                    by = list(doc_id, sentence_id)]

eg_sentences0 <- eg_sentences[, list(text = paste(token, collapse = " ")), 
                                by = list(doc_id,sentence_id)]

eg_sentences0 %>% head() %>% knitr::kable()
```



## Visualizing dependencies

```{r message=FALSE, warning=FALSE}
sentence <- "The green giant wished for Jackie-boy only peace."
sent_depend <- udpipe::udpipe(udmodel, x = sentence)

textplot::textplot_dependencyparser(sent_depend, 
                                    title = sentence, 
                                    subtitle = NULL)
```



## Appendix


